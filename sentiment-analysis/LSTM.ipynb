{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "julia-1.0",
      "display_name": "Julia 1.0"
    },
    "accelerator": "GPU",
    "colab": {
      "name": "[Final] Assignment 2 Q5",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benjmcarthur/deep-learning/blob/master/sentiment-analysis/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7S9cpFJqfXy",
        "colab_type": "text"
      },
      "source": [
        "## Julia on Colaboratory ##\n",
        "\n",
        "[Colaboratory](https://colab.research.google.com) does not provide native support for the [Julia programming language](https://julialang.org). However, since Colaboratory gives you root access to the machine that runs your notebook (the *“runtime”* in Colaboratory terminology), we can install Julia support by uploading a specially crafted Julia notebook  – *this* notebook. We then install Julia and [IJulia](https://github.com/JuliaLang/IJulia.jl) ([Jupyter](https://jupyter.org)/Colaboratory notebook support) and reload the notebook so that Colaboratory detects and initiates what we installed.\n",
        "\n",
        "In brief:\n",
        "\n",
        "1. **Run the cell below**\n",
        "2. **Reload the page**\n",
        "3. **Edit the notebook name and start hacking Julia code below**\n",
        "\n",
        "**If your runtime resets**, either manually or if left idle for some time, **repeat steps 1 and 2**.\n",
        "\n",
        "### Acknowledgements ###\n",
        "\n",
        "This hack by Pontus Stenetorp is an adaptation of [James Bradbury’s original Colaboratory Julia hack](https://discourse.julialang.org/t/julia-on-google-colab-free-gpu-accelerated-shareable-notebooks/15319/27), that broke some time in September 2019 as Colaboratory increased their level of notebook runtime isolation. There also appears to be CUDA compilation support installed by default for each notebook runtime type in October 2019, which shaves off a good 15 minutes or so from the original hack’s installation time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrHjOFFsxf7W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%shell\n",
        "if ! command -v julia 2>&1 > /dev/null\n",
        "then\n",
        "    wget 'https://julialang-s3.julialang.org/bin/linux/x64/1.0/julia-1.0.5-linux-x86_64.tar.gz' \\\n",
        "        -O /tmp/julia.tar.gz\n",
        "    tar -x -f /tmp/julia.tar.gz -C /usr/local --strip-components 1\n",
        "    rm /tmp/julia.tar.gz\n",
        "fi\n",
        "julia -e 'using Pkg; pkg\"add IJulia; precompile;\"'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0n7VXuQ-ahBB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "using Pkg\n",
        "\n",
        "Pkg.add(Pkg.PackageSpec(;name=\"CuArrays\", version=v\"1.3.0\"))\n",
        "Pkg.add(Pkg.PackageSpec(;name=\"Flux\", version=v\"0.9.0\"))\n",
        "\n",
        "pkg\"add MLDatasets\"\n",
        "#pkg\"add ImageMagick\"\n",
        "#pkg\"add Images\"\n",
        "pkg\"add Embeddings\"\n",
        "pkg\"add Plots\"\n",
        "pkg\"precompile\"\n",
        "\n",
        "using CuArrays\n",
        "using Embeddings\n",
        "#using ImageMagick\n",
        "#using Images\n",
        "using Flux\n",
        "using MLDatasets\n",
        "using Plots\n",
        "\n",
        "using LinearAlgebra\n",
        "using Random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8eZSaydKNAc",
        "colab_type": "text"
      },
      "source": [
        "## Stanford Sentiment Treebank ###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3TcLuQztBlz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "run(`curl -fsS https://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip -o /tmp/trainDevTestTrees_PTB.zip`)\n",
        "run(`unzip -q -o -d /tmp /tmp/trainDevTestTrees_PTB.zip`)\n",
        "run(`rm -f /tmp/trainDevTestTrees_PTB.zip`)\n",
        "\n",
        "nothing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw6Mog_3xmVQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "function loadsst(path)\n",
        "    xs = Array{String}[]\n",
        "    ys = Int[]\n",
        "    open(path) do file\n",
        "        # Quick, dirty, and improper S-expression parsing.\n",
        "        for line in eachline(file)\n",
        "            soup = split(line)\n",
        "            push!(ys, parse(Int, lstrip(first(soup), '(')))\n",
        "            tokens = String[]\n",
        "            for chunk in soup[2:end]\n",
        "                endswith(chunk, \")\") || continue\n",
        "                push!(tokens, rstrip(chunk, ')'))\n",
        "            end\n",
        "            push!(xs, tokens)\n",
        "        end\n",
        "    end\n",
        "    xs, ys\n",
        "end\n",
        "\n",
        "ssttrainxs, ssttrainys = loadsst(\"/tmp/trees/train.txt\")\n",
        "sstvalidxs, sstvalidys = loadsst(\"/tmp/trees/dev.txt\")\n",
        "ssttestxs, ssttestys   = loadsst(\"/tmp/trees/test.txt\")\n",
        "\n",
        "nothing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzNU-zpi2eeO",
        "colab_type": "code",
        "outputId": "1384240d-2f5a-481f-86c5-4c25ca6e56f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        }
      },
      "source": [
        "for _ in 1:16\n",
        "    i = rand(1:length(ssttrainxs))\n",
        "    println(\"$(ssttrainys[i]): $(join(ssttrainxs[i], ' '))\")\n",
        "end"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2: Bearable .\n",
            "1: The re - enactments , however fascinating they may be as history , are too crude to serve the work especially well .\n",
            "2: A mess , but it 's a sincere mess .\n",
            "2: aside from showing us in explicit detail how difficult it is to win over the two-drink-minimum crowd , there 's little to be learned from watching ` Comedian '\n",
            "2: This is mostly well-constructed fluff , which is all it seems intended to be .\n",
            "4: It sends you away a believer again and quite cheered at just that .\n",
            "2: What a vast enterprise has been marshaled in the service of such a minute idea .\n",
            "1: The holes in this film remain agape -- holes punched through by an inconsistent , meandering , and sometimes dry plot .\n",
            "1: One Hour Photo may seem disappointing in its generalities , but it 's the little nuances that perhaps had to escape from director Mark Romanek 's self-conscious scrutiny to happen , that finally get under your skin .\n",
            "1: Although it starts off so bad that you feel like running out screaming , it eventually works its way up to merely bad rather than painfully awful .\n",
            "2: This is not Chabrol 's best , but even his lesser works outshine the best some directors can offer .\n",
            "2: The movie is n't horrible , but you can see mediocre cresting on the next wave .\n",
            "1: Has all the values of a straight-to-video movie , but because it has a bigger-name cast , it gets a full theatrical release .\n",
            "4: Leave it to the French to truly capture the terrifying angst of the modern working man without turning the film into a cheap thriller , a dumb comedy or a sappy melodrama .\n",
            "3: Neither the funniest film that Eddie Murphy nor Robert De Niro has ever made , Showtime is nevertheless efficiently amusing for a good while .\n",
            "3: Captures all the longing , anguish and ache , the confusing sexual messages and the wish to be a part of that elusive adult world .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRYlgKo_JVVA",
        "colab_type": "text"
      },
      "source": [
        "## Pre-Processing\n",
        "\n",
        "Load the data, and convert into the correct format for processing in the model.\n",
        "\n",
        "Remove stopwords from each review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmkGmGYizsDQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "function filterCriteria(word)\n",
        "    # Modified from NLTK's list found at https://gist.github.com/sebleier/554280\n",
        "    stopwords = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"you're\", \"you've\", \"you'll\", \"you'd\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"she's\", \"her\", \"hers\", \"herself\", \"it\", \"it's\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"that'll\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"now\", \"d\", \"ll\", \"m\", \"o\", \"re\", \"ve\", \"y\", \"ain\", \"aren\", \"aren't\", \"couldn\", \"couldn't\", \"didn\", \"didn't\", \"doesn\", \"doesn't\", \"hadn\", \"hadn't\", \"hasn\", \"hasn't\", \"haven\", \"haven't\", \"isn\", \"isn't\", \"ma\", \"mightn\", \"mightn't\", \"mustn\", \"mustn't\", \"needn\", \"needn't\", \"shan\", \"shan't\", \"shouldn\", \"shouldn't\", \"wasn\", \"wasn't\", \"weren\", \"weren't\", \"won\", \"won't\", \"wouldn\", \"wouldn't\"]\n",
        "    append!(stopwords, [\",\", \".\", \"--\", \"'d\", \"``\", \"''\", \"`\", \"'\",\"...\", \"?\",\":\",\"it\"])                   \n",
        "    return word ∉ stopwords\n",
        "end\n",
        "    \n",
        "\n",
        "function removeStopwords(data)\n",
        "    processedData = Array{Array{String}}(undef, 0)\n",
        "    for review in data\n",
        "        push!(processedData, filter(filterCriteria, review))\n",
        "    end\n",
        "    return processedData\n",
        "end\n",
        "\n",
        "ssttrainxs = removeStopwords(ssttrainxs)\n",
        "sstvalidxs = removeStopwords(sstvalidxs)\n",
        "ssttestxs  = removeStopwords(ssttestxs);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96OWGH_Ozu-s",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Note that this cell will require the user to approve the download of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sV5ngmpj2Pt",
        "colab_type": "code",
        "outputId": "d3a13cda-aabb-4085-f66d-6034e78d5bdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "using Embeddings\n",
        "const w2v         = load_embeddings(Word2Vec)\n",
        "const token2index = Dict(token => i for (i, token) in enumerate(w2v.vocab))\n",
        "function embedding(token)\n",
        "    # If a token is not in our vocabulary, we use “UNK” to represent it.\n",
        "    token = token in w2v.vocab ? token : \"UNK\"\n",
        "    w2v.embeddings[:, token2index[token]]\n",
        "end\n",
        "\n",
        "nothing"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tcmalloc: large alloc 3600007168 bytes == 0xca1e000 @  0x7fc8da3bfb6b 0x7fc8da3df379 0x7fc8d9cd25ec 0x7fc8d9c9c42b 0x7fc8b324b447 0x7fc8b324a3dc 0x7fc8d9c7fb59 0x7fc8b3249f38 0x7fc8b3249c90 0x7fc8b3249d78 0x7fc8b3244fe9 0x7fc8b32450ab 0x7fc8b3244d8a 0x7fc8d9de8f10 0x7fc8d9de8c39 0x7fc8d9de95bc 0x7fc8d9de9d3f 0x7fc8d9c9596c 0x7fc8d9dea80d 0x7fc8d9cb4ffc 0x7fc8d9c908e0 0x7fc8b3202b8f 0x7fc8b32001dd 0x7fc8d9c7f0d6 0x7fc8d9c8d846 0x7fc8d9c8dea2 0x7fc8c7b9e3e9 0x7fc8c7b9f104 0x7fc8d9c7f0d6 0x7fc8d9c9ae7b (nil)\n",
            "WARNING: redefining constant w2v\n",
            "WARNING: redefining constant token2index\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HH8LzljDNwzi",
        "colab_type": "text"
      },
      "source": [
        "Convert each review into a set of 300-length vectors using the Word2Vec embeddings, then into batches of the correct dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZChYdBj9Jz8r",
        "colab_type": "code",
        "outputId": "a45484f5-c72b-4649-a9ea-5a567407f99a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Converts the raw data into batches\n",
        "function generateDataBatch(data::Array{Array{String}}, batchsize::Int)\n",
        "    # List of batches of size batchsize\n",
        "    # Each batch is 300 x batchsize x n_words\n",
        "    numBatches = Int(ceil(length(data)/batchsize))\n",
        "    encodedData = Array{Array{Float64,3}}(undef,numBatches)\n",
        "    i = 1\n",
        "\n",
        "    # For each batch\n",
        "    for b in 1:numBatches\n",
        "        thisbatchsize = min(i+batchsize-1,length(data))-i+1\n",
        "        batch = zeros(300, thisbatchsize, 60)\n",
        "        n = 1\n",
        "        print(b, \"/\", numBatches,\".. \")\n",
        "        \n",
        "        # For each review in the batch\n",
        "        for r in i:min(i+batchsize-1,length(data))\n",
        "            # For each word\n",
        "            for w in 60:-1:(60-length(data[r])+1)\n",
        "               batch[:,n,w] = embedding(data[r][60-w+1])\n",
        "            end\n",
        "            n += 1\n",
        "        end  \n",
        "        i = min(i+batchsize,length(data))\n",
        "        # Create batch\n",
        "        encodedData[b] = batch\n",
        "    end\n",
        "    println(\" \")\n",
        "    return encodedData\n",
        "end\n",
        "\n",
        "\n",
        "function generateLabelsBatch(data, batchsize)\n",
        "    n = 1\n",
        "    encodedLabels = []\n",
        "    numBatches = Int(ceil(length(data)/batchsize))\n",
        "    for i in 1:numBatches\n",
        "        print(i, \"/\", numBatches,\".. \")\n",
        "        batch = Flux.onehotbatch(data[n:min(n+batchsize-1,length(data))], [0, 1, 2, 3, 4])\n",
        "        push!(encodedLabels, batch)\n",
        "        n = min(n+batchsize,length(data))\n",
        "    end\n",
        "    println(\" \")\n",
        "    return encodedLabels\n",
        "end"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "generateLabelsBatch (generic function with 1 method)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "0529bc47-3390-4170-a0e2-880e49f62a60",
        "id": "xszZbHX-osBz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "# Convert the data into the correct format and load onto the GPU\n",
        "batchsize = 500\n",
        "train_xs = gpu.(generateDataBatch(ssttrainxs, batchsize))\n",
        "val_xs = gpu.(generateDataBatch(sstvalidxs, batchsize))\n",
        "test_xs = gpu.(generateDataBatch(ssttestxs, batchsize))\n",
        "\n",
        "train_ys = gpu.(generateLabelsBatch(ssttrainys, batchsize))\n",
        "val_ys = gpu.(generateLabelsBatch(sstvalidys, batchsize))\n",
        "test_ys = gpu.(generateLabelsBatch(ssttestys, batchsize));"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/18.. 2/18.. 3/18.. 4/18.. 5/18.. 6/18.. 7/18.. 8/18.. 9/18.. 10/18.. 11/18.. 12/18.. 13/18.. 14/18.. 15/18.. 16/18.. 17/18.. 18/18..  \n",
            "1/3.. 2/3.. 3/3..  \n",
            "1/5.. 2/5.. 3/5.. 4/5.. 5/5..  \n",
            "1/18.. 2/18.. 3/18.. 4/18.. 5/18.. 6/18.. 7/18.. 8/18.. 9/18.. 10/18.. 11/18.. 12/18.. 13/18.. 14/18.. 15/18.. 16/18.. 17/18.. 18/18..  \n",
            "1/3.. 2/3.. 3/3..  \n",
            "1/5.. 2/5.. 3/5.. 4/5.. 5/5..  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpRA4uU8KC46",
        "colab_type": "text"
      },
      "source": [
        "## 5.1 Implement a Recurrent Neural Network model\n",
        "\n",
        "*Implement a recurrent neural network model which encodes a sequence of words into inputs to a multi-layer perceptron with a cross-entropy loss for the Stanford Sentiment Treebank data.*\n",
        "\n",
        "The neural network model in this question uses a two-stage sequence to encode and then classify movie reviews by their ratings.\n",
        "\n",
        "* **Accumulator**: this is a LSTM cell which is fed the sequence of words, in order. The final output of the LSTM is then fed into the classifier.\n",
        "* **Classifier**: a MLP which takes the final output of the Accumulator and classifies it into one of the five ratings.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2229iObFJ81A",
        "colab_type": "code",
        "outputId": "2f342a6f-b0b6-4f87-bc02-cdc89f673b5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "using Flux\n",
        "\n",
        "# Accumulator layer takes 1 word and re-codes it with respect to the previous memory\n",
        "# Classifier layer classifies the encoded word into a rating\n",
        "\n",
        "# Set up model\n",
        "accumulator = LSTM(300,120) |> gpu\n",
        "classifier = Chain( Dense(120,120, leakyrelu),\n",
        "                    Dense(120, 5, leakyrelu),\n",
        "                    softmax) |> gpu\n",
        "model = Chain(accumulator, classifier)\n",
        "\n",
        "#Loss function\n",
        "function loss(xbatch, ybatch)\n",
        "    n_words = size(xbatch)[3]\n",
        "    Flux.reset!(accumulator)\n",
        "    for w in 1:n_words-1\n",
        "        accumulator(xbatch[:,:,w])\n",
        "    end\n",
        "    l = Flux.crossentropy(classifier(accumulator(xbatch[:,:,n_words])), ybatch)\n",
        "    return l/size(xbatch)[2]\n",
        "end"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "loss (generic function with 1 method)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgwfco-MK2R_",
        "colab_type": "text"
      },
      "source": [
        "## 5.2 Explore model variants\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5L7ScMXxhvQs",
        "colab_type": "text"
      },
      "source": [
        "### 5.2.1 Recurrent Units\n",
        "\n",
        "Three types of recurrent units were tested, each with the same MLP classifier:\n",
        "* LSTM\n",
        "* GRU\n",
        "* RNN\n",
        "\n",
        "The table below summarises the results:\n",
        "\n",
        "|                      | LSTM    | GRU     | RNN     |\n",
        "|----------------------|---------|---------|---------|\n",
        "| Best validation loss | 0.00583 | 0.00592 | 0.00641 |\n",
        "\n",
        "The plots of the validation loss vs epochs during training showed that:\n",
        "* The GRU learned quickly but was prone to overfitting early\n",
        "* The RNN did not learn at all, compared to the LSTM or the GRU.\n",
        "\n",
        "Based on this analysis, the LSTM was the best recurrent cell to continue with. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKiut-HIz6DF",
        "colab_type": "text"
      },
      "source": [
        "### 5.2.2 MLP Size\n",
        "\n",
        "Various permutations of the number and dimensions of layers were trialled. The table below summarises the results.\n",
        "\n",
        "\n",
        "|  MLP Structure | Validation Accuracy |\n",
        "|:--------------:|:-------------------:|\n",
        "|   50-30, 30-5  |       42.2343       |\n",
        "|   50-50, 50-5  |       43.0518       |\n",
        "|   60-60, 60-5  |       43.8692       |\n",
        "| 120-100, 100-5 |       44.3233       |\n",
        "| 120-120, 120-5 |       45.0500       |\n",
        "| 150-120, 120-5 |       42.9609       |\n",
        "| 200-100, 100-5 |       43.6876       |\n",
        "| 150-150, 150-5 |       43.7784       |\n",
        "| 200-200, 200-5 |       41.6894       |\n",
        "\n",
        "It was obvious that larger models were relatively quick to overfit, as shown by the learning graph below for a MLP of structure 200-100,100-5. \n",
        "\n",
        "![Overfitting](https://drive.google.com/uc?export=view&id=1QizEraX0Pf7yGgRRo4hOE05lADXDPtAx)\n",
        "\n",
        "As with all machine learning problems, a balance is needed between model complexity and avoiding overfitting. Dropout was examined as one technique for avoiding overfitting, which is discussed in Section 5.2.6.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wdJkWeBepfG",
        "colab_type": "text"
      },
      "source": [
        "### 5.2.3 Stopwords\n",
        "\n",
        "It is common in Natural Language Processing to remove punctuation and other words which are not likely to add to understanding the sentiment of a sentence. The effectiveness of doing this in this context was evaluated by using the below code to remove words from each review."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8w2nLxAgmbA",
        "colab_type": "code",
        "outputId": "9ea30d71-28c4-4999-b4a8-6f403682d3c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "function filterCriteria(word)\n",
        "    # Modified from NLTK's list found at https://gist.github.com/sebleier/554280\n",
        "    stopwords = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"you're\", \"you've\", \"you'll\", \"you'd\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"she's\", \"her\", \"hers\", \"herself\", \"it\", \"it's\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"that'll\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"now\", \"d\", \"ll\", \"m\", \"o\", \"re\", \"ve\", \"y\", \"ain\", \"aren\", \"aren't\", \"couldn\", \"couldn't\", \"didn\", \"didn't\", \"doesn\", \"doesn't\", \"hadn\", \"hadn't\", \"hasn\", \"hasn't\", \"haven\", \"haven't\", \"isn\", \"isn't\", \"ma\", \"mightn\", \"mightn't\", \"mustn\", \"mustn't\", \"needn\", \"needn't\", \"shan\", \"shan't\", \"shouldn\", \"shouldn't\", \"wasn\", \"wasn't\", \"weren\", \"weren't\", \"won\", \"won't\", \"wouldn\", \"wouldn't\"]\n",
        "    append!(stopwords, [\",\", \".\", \"--\", \"'d\", \"``\", \"''\", \"`\", \"'\",\"...\", \"?\",\":\",\"it\"])                   \n",
        "    return word ∉ stopwords\n",
        "end\n",
        "    \n",
        "\n",
        "function removeStopwords(data::Array{Array{String}})\n",
        "    processedData = Array{Array{String}}(undef, 0)\n",
        "    for review in data\n",
        "        push!(processedData, filter(filterCriteria, review))\n",
        "    end\n",
        "    return processedData\n",
        "end"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "removeStopwords (generic function with 2 methods)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8P2zo5ynAWA",
        "colab_type": "text"
      },
      "source": [
        "The table below summarises the results. The model trained on the full sentences performed slightly worse than the model trained on the shortened sentences.\n",
        "\n",
        "|                              | Stopwords Removed | Full Sentences    |\n",
        "|------------------------------|-------------------|-------------------|\n",
        "| Best validation accuracy (%) | 43.0518           | 41.4169           |\n",
        "| Best validation loss         | 0.005833          | 0.005907          |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIvLTIi7Obdm",
        "colab_type": "text"
      },
      "source": [
        "### 5.2.4 Fixed or Variable Word Embeddings\n",
        "\n",
        "For the majority of the testing the embeddings provided by word2vec have been used as-is and have not been updated during training. However, there is potential for some improvement to the accuracy by updating the embeddings as the model \"learns\" more effective embeddings for this specific purpose.\n",
        "\n",
        "The code used for this is below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xX8UG-hUCZ2",
        "colab_type": "text"
      },
      "source": [
        "Batching functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYyftxwHSnoh",
        "colab_type": "code",
        "outputId": "2af60cb0-78ec-497c-b926-531ba6e64d90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Create training data in correct format\n",
        "\n",
        "function vocabEmbed(token)\n",
        "    token = token in w2v.vocab ? token : \"UNK\"\n",
        "    return Flux.onehot(token2index[token], 1:size(w2v.vocab)[1])\n",
        "end\n",
        "\n",
        "function generateDataVocab(data::Array{Array{String}}, batchsize::Int)\n",
        "    # List of batches of size batchsize\n",
        "    # Each batch is 300 x batchsize x n_words\n",
        "    numBatches = Int(ceil(length(data)/batchsize))\n",
        "    encodedData = Array{Array{Float64,3}}(undef,numBatches)\n",
        "    i = 1\n",
        "\n",
        "    # For each batch\n",
        "    for b in 1:numBatches\n",
        "        thisbatchsize = min(i+batchsize-1,length(data))-i+1\n",
        "        batch = Array{Float64,3}(undef, 929022, thisbatchsize, 60)\n",
        "        println(\"Batch size: \", size(batch))\n",
        "        n = 1\n",
        "        print(b, \"/\", numBatches,\".. \")\n",
        "        \n",
        "        # For each review in the batch\n",
        "        for r in i:min(i+batchsize-1,length(data))\n",
        "            # For each word\n",
        "            for w in 60:-1:(60-length(data[r])+1)\n",
        "               batch[:,n,w] = vocabEmbed(data[r][60-w+1])\n",
        "            end\n",
        "            n += 1\n",
        "        end  \n",
        "        i = min(i+batchsize,length(data))\n",
        "        #push!(encodedData2, batch)\n",
        "        encodedData[b] = batch\n",
        "    end\n",
        "    println(\" \")\n",
        "    return encodedData2\n",
        "end"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "generateDataVocab (generic function with 1 method)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RDyTjRJT6BV",
        "colab_type": "text"
      },
      "source": [
        "Loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rr5TQrgMTyFZ",
        "colab_type": "code",
        "outputId": "8e2cb7c3-1862-49f7-f237-53c7063e7d6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "function loss_embed(xbatch, ybatch)\n",
        "    n_words = size(xbatch)[3]\n",
        "    batchsize = size(xbatch)[2]\n",
        "    temp = mutableEmbeddings*reshape(xBatch, (929022,:))\n",
        "    xEmbed = reshape(temp,(300,batchsize,n_words))\n",
        "            \n",
        "    Flux.reset!(accumulator)\n",
        "    for w in 1:n_words-1\n",
        "        accumulator(xEmbed[:,:,w])\n",
        "    end\n",
        "    l = Flux.crossentropy(classifier(accumulator(xEmbed[:,:,n_words])), ybatch)\n",
        "    return l\n",
        "end"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "loss_embed (generic function with 1 method)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNaguUyDOH7d",
        "colab_type": "text"
      },
      "source": [
        "Parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2LFVkjFUAkQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mutableEmbeddings = w2v.embeddings\n",
        "params_embed = Flux.params(mutableEmbeddings, accumulator, classifier);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04YHDGrYYpIk",
        "colab_type": "text"
      },
      "source": [
        "However, experimenting with this technique did not improve the model's performance, so it was excluded from the final model. Several issues were encountered where the Colabatory notebooks didn't have enough RAM to store these significantly larger arrays."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlqwqBuvX5lH",
        "colab_type": "text"
      },
      "source": [
        "### 5.2.5 Ordinal Labelling\n",
        "\n",
        "Because the labels are ordinal rankings, rather than completely discrete categories, a higher accuracy may be achieved through embedding some of this information into the labels.\n",
        "\n",
        "A technique proposed in \"A neural network approach to ordinal regression.\" by Cheng, Jianlin, Zheng Wang, and Gianluca Pollastri proposed encoding vectors as a series of $k$ ones followed by $d-k$ zeros, where k is the labelled class and d is the total number of classes. \n",
        "\n",
        "For example, for our data, a review labelled 1 was encoded as `[1 0 0 0 0]` and a review labelled 4 was encoded as `[1 1 1 1 0]`. \n",
        "\n",
        "The underlying model was required to be changed from a softmax output to a sigmoid output, and the loss function was changed to the mean squared error between the predicted and actual vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEP7tlIRYGWL",
        "colab_type": "code",
        "outputId": "37b46e68-3b53-49d2-bfba-676a053aa518",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "function generateLabelsOrdinal(data, batchsize)\n",
        "    n = 1\n",
        "    encodedLabels = []\n",
        "    numBatches = Int(ceil(length(data)/batchsize))\n",
        "    for b in 1:numBatches\n",
        "        print(b, \"/\", numBatches,\".. \")\n",
        "        thisbatchsize = min(n+batchsize-1,length(data))-n+1\n",
        "        batch = zeros(Bool,5,thisbatchsize)\n",
        "        i = 1\n",
        "        for r in n:min(n+batchsize-1,length(data))\n",
        "            for j = 1:data[r]+1\n",
        "                batch[j,i] = 1\n",
        "            end\n",
        "            i += 1\n",
        "        end\n",
        "        push!(encodedLabels, batch)\n",
        "        n = min(n+batchsize,length(data))\n",
        "    end\n",
        "    println(\" \")\n",
        "    return encodedLabels\n",
        "end"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "generateLabelsOrdinal (generic function with 1 method)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bri83fiijNRD",
        "colab_type": "code",
        "outputId": "34a8fc04-ce94-4119-8e80-c999e788df58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Set up model\n",
        "sig(x) = (1/(1+exp(x)))\n",
        "accumulator_ordinal = LSTM(300,50) |> gpu\n",
        "classifier_ordinal = Chain( Dense(50,50),\n",
        "                    Dense(50,  5, leakyrelu),\n",
        "                    x -> (sig.(x))) |> gpu\n",
        "model_ordinal = Chain(accumulator_ordinal, classifier_ordinal)\n",
        "\n",
        "#Loss function\n",
        "function loss_ordinal(xbatch, ybatch)\n",
        "    n_words = size(xbatch)[3]\n",
        "    Flux.reset!(accumulator_ordinal)\n",
        "    for w in 1:n_words-1\n",
        "        accumulator_ordinal(xbatch[:,:,w])\n",
        "    end\n",
        "    l = Flux.mse(classifier_ordinal(accumulator_ordinal(xbatch[:,:,n_words])), ybatch)\n",
        "    return l/size(xbatch)[2]\n",
        "end"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "loss_ordinal (generic function with 1 method)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJUrZmYcYP2x",
        "colab_type": "code",
        "outputId": "2ff2ddd1-9dbe-42f2-e9b5-f6c781f6c379",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Define a new accuracy function\n",
        "\n",
        "function findPredictedOrdinal(x)\n",
        "    for i in 2:5\n",
        "        if x[i] < 0.5\n",
        "          return i-1\n",
        "        else\n",
        "          return 5\n",
        "        end\n",
        "    end\n",
        "end\n",
        "\n",
        "function ordinalAccuracy(xdata, ydata)\n",
        "    c = 0\n",
        "    n = 0\n",
        "    for batch in 1:length(xdata)\n",
        "      xbatch = xdata[batch]\n",
        "      n_words = size(xbatch)[3]\n",
        "      Flux.reset!(accumulator)\n",
        "      for w in 1:n_words-1\n",
        "          accumulator(xbatch[:,:,w])\n",
        "      end\n",
        "      output = (classifier(accumulator(xbatch[:,:,n_words]))).data\n",
        "      predicted = []\n",
        "      for x in 1:size(output)[2]\n",
        "          predicted = findPredictedOrdinal(output[:,x])\n",
        "          #println(\"Predicted: \", predicted)\n",
        "          actual = Int(sum(ydata[batch][:,x]))\n",
        "          #println(\"Actual: \", actual)\n",
        "          if predicted == actual\n",
        "              #println(\"Yes!!\")\n",
        "              c += 1\n",
        "          end\n",
        "         # println(\" \")\n",
        "          n += 1\n",
        "      end\n",
        "    end\n",
        "    return (c/n)*100\n",
        "end"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ordinalAccuracy (generic function with 1 method)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-AGV5NRsFa2",
        "colab_type": "text"
      },
      "source": [
        "However, the results were not promising, with a maximum validation accuracy achieved of 14.9864. This method is not used in the final model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNiEly6E1z6t",
        "colab_type": "text"
      },
      "source": [
        "### 5.2.6 Dropout\n",
        "\n",
        "As a way of avoiding overfitting while using more complex MLP's, the performance of several networks was compared with and without dropout. In general, for larger networks the dropout helped to improve the accuracy slightly, while for smaller networks it had a neutral or negative effect.\n",
        "\n",
        "| MLP Structure | Dropout rate | Accuracy without dropout | Accuracy with dropout |\n",
        "|---------------|--------------|--------------------------|-----------------------|\n",
        "| 60-5          | 0.01         | 43.0613                  | 43.0518               |\n",
        "| 50-30, 30-5   | 0.1          | 42.2343                  | 41.9619               |\n",
        "| 60-60, 60-5   | 0.05         | 43.0518                  | 43.8692               |\n",
        "| 300-200,200-5 | 0.1          | 41.6894                  | 42.0237               |\n",
        "\n",
        "The graph below shows the training of the `[300-200, 200-5]` MLP with dropout, which helps prevent overfitting until epoch 150.\n",
        "\n",
        "![Overfitting](https://drive.google.com/uc?export=view&id=1lY98NczGgZtazS8aXoIVzn7ptSB9ZbZj)\n",
        "\n",
        "\n",
        "The graph below shows the training of the `[60-5]` MLP where dropout doesn't improve the solution but introduces some randomness to the training, particularly to the loss on the validation set.\n",
        "\n",
        "![Overfitting](https://drive.google.com/uc?export=view&id=13jNnpEtwkdwhV-6Ydzdsmzl3H7CWIn_y)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwEez6ncr0aS",
        "colab_type": "text"
      },
      "source": [
        "### 5.2.7 Regularisation\n",
        "\n",
        "Another method to prevent overfitting is the use of regularisation. To do this the loss function is modified to include a term which penalises parameters taking large values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKM894dosF-2",
        "colab_type": "code",
        "outputId": "2bd9eb9b-33f3-4c79-a59a-e4531335a9a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Loss function\n",
        "function loss_reg(xbatch, ybatch)\n",
        "    lambda = 0.001\n",
        "    n_words = size(xbatch)[3]\n",
        "    Flux.reset!(accumulator)\n",
        "    for w in 1:n_words-1\n",
        "        accumulator(xbatch[:,:,w])\n",
        "    end\n",
        "    l = Flux.crossentropy(classifier(accumulator(xbatch[:,:,n_words])), ybatch) + lambda*sum(norm, Flux.params(classifier))\n",
        "    return l/size(xbatch)[2]\n",
        "end"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "loss_reg (generic function with 1 method)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vO2TU0qvMAw",
        "colab_type": "text"
      },
      "source": [
        "We found that regularisation was effective at reducing overfitting for larger networks, but for smaller networks it only served to reduce the overall accuracy of the model. \n",
        "\n",
        "For a MLP with size `[200-100], [100-5]`, the table below shows that a small regularisation served to increase the overall validation accuracy.\n",
        "\n",
        "| Regularisation Coefficient | Maximum Validation Accuracy |\n",
        "|:--------------------------:|:---------------------------:|\n",
        "|              0             |            41.14            |\n",
        "|            0.01            |            42.42            |\n",
        "|             0.1            |            26.25            |\n",
        "\n",
        "For a MLP with size `[60-60], [60-5]`, any regularisation decreased the validation accuracy.\n",
        "\n",
        "| Regularisation Coefficient | Maximum Validation Accuracy |\n",
        "|:--------------------------:|:---------------------------:|\n",
        "|              0             |            43.87            |\n",
        "|            0.005           |            40.78            |\n",
        "|            0.05            |            39.24            |\n",
        "\n",
        "Even with regularisation of the larger MLP's, the smaller MLP's still had the best validation performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c07Ot-qELAox",
        "colab_type": "text"
      },
      "source": [
        "## 5.3 Train the final model to convergence\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0V5jLQxBwkC0",
        "colab_type": "text"
      },
      "source": [
        "The final model has the following settings: \n",
        "* A LSTM recurrent unit transmitting from a 300-length input vector to 60-length output vector\n",
        "* A two-layer MLP with layer sizes `[120-120]` and `[120-5]`\n",
        "* No dropout or regularisation\n",
        "* Stopwords removed\n",
        "* Fixed word embeddings\n",
        "* Non-ordinal labelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZNid2PbLGTj",
        "colab_type": "code",
        "outputId": "0d412a4f-de83-45ba-dbc3-f39a3fb923f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "using Flux\n",
        "\n",
        "Flux.testmode!(classifier, false)\n",
        "best_params_a = undef\n",
        "best_params_c = undef\n",
        "best_val_loss = 100\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "n_epochs = 200\n",
        "\n",
        "evalcb() = nothing\n",
        "params = Flux.params(accumulator, classifier)\n",
        "opt = Flux.ADAM()\n",
        "\n",
        "start_time = time()\n",
        "prev_time = time()\n",
        "for epoch in 1:n_epochs\n",
        "\n",
        "    # Print diagnostics\n",
        "    println(\"Time for epoch: \", time() - prev_time)\n",
        "    prev_time = time()\n",
        "    println(\" \")\n",
        "    println(\" \")\n",
        "    println(\"Starting epoch: \", epoch)\n",
        "\n",
        "    # Train model\n",
        "    Flux.train!(loss, params, zip(train_xs, train_ys), opt, cb = Flux.throttle(evalcb, 5))\n",
        "\n",
        "    # Track losses\n",
        "    train_loss = (sum(loss.(train_xs, train_ys))/length(train_xs)).data\n",
        "    val_loss   = (sum(loss.(val_xs, val_ys))/length(val_xs)).data\n",
        "    push!(train_losses, train_loss)\n",
        "    push!(val_losses,   val_loss)\n",
        "    println(\"Validation loss: \", val_loss)\n",
        "    println(\"Best validation loss: \", best_val_loss)\n",
        "\n",
        "    # Save best model weights\n",
        "    if val_loss < best_val_loss\n",
        "      println(\"Found new best parameters!\")\n",
        "      #best_params_e = deepcopy(Tracker.data.(Flux.params(mutableEmbeddings)))\n",
        "      best_params_a = deepcopy(Tracker.data.(Flux.params(accumulator)))\n",
        "      best_params_c = deepcopy(Tracker.data.(Flux.params(classifier)))\n",
        "      best_val_loss = val_loss\n",
        "    end\n",
        "\n",
        "end\n",
        "println(\"Time for epoch: \", time() - prev_time)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time for epoch: 1.8857698440551758\n",
            " \n",
            " \n",
            "Starting epoch: 1\n",
            "Validation loss: 0.007456079\n",
            "Best validation loss: 100\n",
            "Found new best parameters!\n",
            "Time for epoch: 31.7619788646698\n",
            " \n",
            " \n",
            "Starting epoch: 2\n",
            "Validation loss: 0.007452277\n",
            "Best validation loss: 0.007456079\n",
            "Found new best parameters!\n",
            "Time for epoch: 1.303537130355835\n",
            " \n",
            " \n",
            "Starting epoch: 3\n",
            "Validation loss: 0.007451646\n",
            "Best validation loss: 0.007452277\n",
            "Found new best parameters!\n",
            "Time for epoch: 1.3567051887512207\n",
            " \n",
            " \n",
            "Starting epoch: 4\n",
            "Validation loss: 0.006507943\n",
            "Best validation loss: 0.007451646\n",
            "Found new best parameters!\n",
            "Time for epoch: 1.4800851345062256\n",
            " \n",
            " \n",
            "Starting epoch: 5\n",
            "Validation loss: 0.0070601064\n",
            "Best validation loss: 0.006507943\n",
            "Time for epoch: 1.2841949462890625\n",
            " \n",
            " \n",
            "Starting epoch: 6\n",
            "Validation loss: 0.006847291\n",
            "Best validation loss: 0.006507943\n",
            "Time for epoch: 1.2518630027770996\n",
            " \n",
            " \n",
            "Starting epoch: 7\n",
            "Validation loss: 0.0069315345\n",
            "Best validation loss: 0.006507943\n",
            "Time for epoch: 1.2895419597625732\n",
            " \n",
            " \n",
            "Starting epoch: 8\n",
            "Validation loss: 0.0068337363\n",
            "Best validation loss: 0.006507943\n",
            "Time for epoch: 1.337831974029541\n",
            " \n",
            " \n",
            "Starting epoch: 9\n",
            "Validation loss: 0.006848103\n",
            "Best validation loss: 0.006507943\n",
            "Time for epoch: 1.270550012588501\n",
            " \n",
            " \n",
            "Starting epoch: 10\n",
            "Validation loss: 0.0068201\n",
            "Best validation loss: 0.006507943\n",
            "Time for epoch: 1.2479920387268066\n",
            " \n",
            " \n",
            "Starting epoch: 11\n",
            "Validation loss: 0.006735088\n",
            "Best validation loss: 0.006507943\n",
            "Time for epoch: 1.250223159790039\n",
            " \n",
            " \n",
            "Starting epoch: 12\n",
            "Validation loss: 0.0066821086\n",
            "Best validation loss: 0.006507943\n",
            "Time for epoch: 1.2790708541870117\n",
            " \n",
            " \n",
            "Starting epoch: 13\n",
            "Validation loss: 0.006356399\n",
            "Best validation loss: 0.006507943\n",
            "Found new best parameters!\n",
            "Time for epoch: 1.2967948913574219\n",
            " \n",
            " \n",
            "Starting epoch: 14\n",
            "Validation loss: 0.006717359\n",
            "Best validation loss: 0.006356399\n",
            "Time for epoch: 1.2646739482879639\n",
            " \n",
            " \n",
            "Starting epoch: 15\n",
            "Validation loss: 0.006514427\n",
            "Best validation loss: 0.006356399\n",
            "Time for epoch: 1.2897210121154785\n",
            " \n",
            " \n",
            "Starting epoch: 16\n",
            "Validation loss: 0.0066910232\n",
            "Best validation loss: 0.006356399\n",
            "Time for epoch: 1.2830400466918945\n",
            " \n",
            " \n",
            "Starting epoch: 17\n",
            "Validation loss: 0.0068966276\n",
            "Best validation loss: 0.006356399\n",
            "Time for epoch: 1.3232131004333496\n",
            " \n",
            " \n",
            "Starting epoch: 18\n",
            "Validation loss: 0.0065305936\n",
            "Best validation loss: 0.006356399\n",
            "Time for epoch: 1.2836620807647705\n",
            " \n",
            " \n",
            "Starting epoch: 19\n",
            "Validation loss: 0.0062951813\n",
            "Best validation loss: 0.006356399\n",
            "Found new best parameters!\n",
            "Time for epoch: 1.3388869762420654\n",
            " \n",
            " \n",
            "Starting epoch: 20\n",
            "Validation loss: 0.0063061756\n",
            "Best validation loss: 0.0062951813\n",
            "Time for epoch: 1.3050990104675293\n",
            " \n",
            " \n",
            "Starting epoch: 21\n",
            "Validation loss: 0.00617581\n",
            "Best validation loss: 0.0062951813\n",
            "Found new best parameters!\n",
            "Time for epoch: 1.318274974822998\n",
            " \n",
            " \n",
            "Starting epoch: 22\n",
            "Validation loss: 0.0062208255\n",
            "Best validation loss: 0.00617581\n",
            "Time for epoch: 1.3077960014343262\n",
            " \n",
            " \n",
            "Starting epoch: 23\n",
            "Validation loss: 0.0062160506\n",
            "Best validation loss: 0.00617581\n",
            "Time for epoch: 1.5284860134124756\n",
            " \n",
            " \n",
            "Starting epoch: 24\n",
            "Validation loss: 0.006187258\n",
            "Best validation loss: 0.00617581\n",
            "Time for epoch: 1.3138179779052734\n",
            " \n",
            " \n",
            "Starting epoch: 25\n",
            "Validation loss: 0.0062452983\n",
            "Best validation loss: 0.00617581\n",
            "Time for epoch: 1.267529010772705\n",
            " \n",
            " \n",
            "Starting epoch: 26\n",
            "Validation loss: 0.006151315\n",
            "Best validation loss: 0.00617581\n",
            "Found new best parameters!\n",
            "Time for epoch: 1.300889015197754\n",
            " \n",
            " \n",
            "Starting epoch: 27\n",
            "Validation loss: 0.006213414\n",
            "Best validation loss: 0.006151315\n",
            "Time for epoch: 1.2660088539123535\n",
            " \n",
            " \n",
            "Starting epoch: 28\n",
            "Validation loss: 0.0061122305\n",
            "Best validation loss: 0.006151315\n",
            "Found new best parameters!\n",
            "Time for epoch: 1.29066801071167\n",
            " \n",
            " \n",
            "Starting epoch: 29\n",
            "Validation loss: 0.0061250795\n",
            "Best validation loss: 0.0061122305\n",
            "Time for epoch: 1.2976069450378418\n",
            " \n",
            " \n",
            "Starting epoch: 30\n",
            "Validation loss: 0.0060854354\n",
            "Best validation loss: 0.0061122305\n",
            "Found new best parameters!\n",
            "Time for epoch: 1.3539390563964844\n",
            " \n",
            " \n",
            "Starting epoch: 31\n",
            "Validation loss: 0.0061176135\n",
            "Best validation loss: 0.0060854354\n",
            "Time for epoch: 1.2752249240875244\n",
            " \n",
            " \n",
            "Starting epoch: 32\n",
            "Validation loss: 0.006002169\n",
            "Best validation loss: 0.0060854354\n",
            "Found new best parameters!\n",
            "Time for epoch: 1.2852110862731934\n",
            " \n",
            " \n",
            "Starting epoch: 33\n",
            "Validation loss: 0.0060898564\n",
            "Best validation loss: 0.006002169\n",
            "Time for epoch: 1.2909481525421143\n",
            " \n",
            " \n",
            "Starting epoch: 34\n",
            "Validation loss: 0.0060498775\n",
            "Best validation loss: 0.006002169\n",
            "Time for epoch: 1.3089649677276611\n",
            " \n",
            " \n",
            "Starting epoch: 35\n",
            "Validation loss: 0.006040657\n",
            "Best validation loss: 0.006002169\n",
            "Time for epoch: 1.2835280895233154\n",
            " \n",
            " \n",
            "Starting epoch: 36\n",
            "Validation loss: 0.0059579103\n",
            "Best validation loss: 0.006002169\n",
            "Found new best parameters!\n",
            "Time for epoch: 1.295590877532959\n",
            " \n",
            " \n",
            "Starting epoch: 37\n",
            "Validation loss: 0.005988253\n",
            "Best validation loss: 0.0059579103\n",
            "Time for epoch: 1.3106529712677002\n",
            " \n",
            " \n",
            "Starting epoch: 38\n",
            "Validation loss: 0.005981691\n",
            "Best validation loss: 0.0059579103\n",
            "Time for epoch: 1.3191111087799072\n",
            " \n",
            " \n",
            "Starting epoch: 39\n",
            "Validation loss: 0.005943985\n",
            "Best validation loss: 0.0059579103\n",
            "Found new best parameters!\n",
            "Time for epoch: 1.2696199417114258\n",
            " \n",
            " \n",
            "Starting epoch: 40\n",
            "Validation loss: 0.005933006\n",
            "Best validation loss: 0.005943985\n",
            "Found new best parameters!\n",
            "Time for epoch: 1.2935020923614502\n",
            " \n",
            " \n",
            "Starting epoch: 41\n",
            "Validation loss: 0.0059350245\n",
            "Best validation loss: 0.005933006\n",
            "Time for epoch: 1.2510650157928467\n",
            " \n",
            " \n",
            "Starting epoch: 42\n",
            "Validation loss: 0.0059170015\n",
            "Best validation loss: 0.005933006\n",
            "Found new best parameters!\n",
            "Time for epoch: 1.3274118900299072\n",
            " \n",
            " \n",
            "Starting epoch: 43\n",
            "Validation loss: 0.0058818855\n",
            "Best validation loss: 0.0059170015\n",
            "Found new best parameters!\n",
            "Time for epoch: 1.2830040454864502\n",
            " \n",
            " \n",
            "Starting epoch: 44\n",
            "Validation loss: 0.005916711\n",
            "Best validation loss: 0.0058818855\n",
            "Time for epoch: 1.2939260005950928\n",
            " \n",
            " \n",
            "Starting epoch: 45\n",
            "Validation loss: 0.005919846\n",
            "Best validation loss: 0.0058818855\n",
            "Time for epoch: 1.288032054901123\n",
            " \n",
            " \n",
            "Starting epoch: 46\n",
            "Validation loss: 0.005875509\n",
            "Best validation loss: 0.0058818855\n",
            "Found new best parameters!\n",
            "Time for epoch: 1.3301539421081543\n",
            " \n",
            " \n",
            "Starting epoch: 47\n",
            "Validation loss: 0.005859047\n",
            "Best validation loss: 0.005875509\n",
            "Found new best parameters!\n",
            "Time for epoch: 1.2508699893951416\n",
            " \n",
            " \n",
            "Starting epoch: 48\n",
            "Validation loss: 0.005891709\n",
            "Best validation loss: 0.005859047\n",
            "Time for epoch: 1.2692270278930664\n",
            " \n",
            " \n",
            "Starting epoch: 49\n",
            "Validation loss: 0.0059258253\n",
            "Best validation loss: 0.005859047\n",
            "Time for epoch: 1.2550268173217773\n",
            " \n",
            " \n",
            "Starting epoch: 50\n",
            "Validation loss: 0.005868418\n",
            "Best validation loss: 0.005859047\n",
            "Time for epoch: 1.2700412273406982\n",
            " \n",
            " \n",
            "Starting epoch: 51\n",
            "Validation loss: 0.005869836\n",
            "Best validation loss: 0.005859047\n",
            "Time for epoch: 1.2653930187225342\n",
            " \n",
            " \n",
            "Starting epoch: 52\n",
            "Validation loss: 0.005839121\n",
            "Best validation loss: 0.005859047\n",
            "Found new best parameters!\n",
            "Time for epoch: 1.2612919807434082\n",
            " \n",
            " \n",
            "Starting epoch: 53\n",
            "Validation loss: 0.0058583375\n",
            "Best validation loss: 0.005839121\n",
            "Time for epoch: 1.3009579181671143\n",
            " \n",
            " \n",
            "Starting epoch: 54\n",
            "Validation loss: 0.005880946\n",
            "Best validation loss: 0.005839121\n",
            "Time for epoch: 1.2842028141021729\n",
            " \n",
            " \n",
            "Starting epoch: 55\n",
            "Validation loss: 0.00591525\n",
            "Best validation loss: 0.005839121\n",
            "Time for epoch: 1.2590670585632324\n",
            " \n",
            " \n",
            "Starting epoch: 56\n",
            "Validation loss: 0.0058948654\n",
            "Best validation loss: 0.005839121\n",
            "Time for epoch: 1.2828810214996338\n",
            " \n",
            " \n",
            "Starting epoch: 57\n",
            "Validation loss: 0.005903522\n",
            "Best validation loss: 0.005839121\n",
            "Time for epoch: 1.2661561965942383\n",
            " \n",
            " \n",
            "Starting epoch: 58\n",
            "Validation loss: 0.0059084645\n",
            "Best validation loss: 0.005839121\n",
            "Time for epoch: 1.298003911972046\n",
            " \n",
            " \n",
            "Starting epoch: 59\n",
            "Validation loss: 0.0058900616\n",
            "Best validation loss: 0.005839121\n",
            "Time for epoch: 1.27012300491333\n",
            " \n",
            " \n",
            "Starting epoch: 60\n",
            "Validation loss: 0.0058643282\n",
            "Best validation loss: 0.005839121\n",
            "Time for epoch: 1.2733960151672363\n",
            " \n",
            " \n",
            "Starting epoch: 61\n",
            "Validation loss: 0.005739489\n",
            "Best validation loss: 0.005839121\n",
            "Found new best parameters!\n",
            "Time for epoch: 1.260308027267456\n",
            " \n",
            " \n",
            "Starting epoch: 62\n",
            "Validation loss: 0.0058748424\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.3221299648284912\n",
            " \n",
            " \n",
            "Starting epoch: 63\n",
            "Validation loss: 0.0057782703\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2964110374450684\n",
            " \n",
            " \n",
            "Starting epoch: 64\n",
            "Validation loss: 0.0058843433\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2775659561157227\n",
            " \n",
            " \n",
            "Starting epoch: 65\n",
            "Validation loss: 0.0059132134\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.5050749778747559\n",
            " \n",
            " \n",
            "Starting epoch: 66\n",
            "Validation loss: 0.0060254037\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.3463690280914307\n",
            " \n",
            " \n",
            "Starting epoch: 67\n",
            "Validation loss: 0.005994992\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.3180949687957764\n",
            " \n",
            " \n",
            "Starting epoch: 68\n",
            "Validation loss: 0.005964238\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.285236120223999\n",
            " \n",
            " \n",
            "Starting epoch: 69\n",
            "Validation loss: 0.005998323\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2716610431671143\n",
            " \n",
            " \n",
            "Starting epoch: 70\n",
            "Validation loss: 0.0059501803\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.3165550231933594\n",
            " \n",
            " \n",
            "Starting epoch: 71\n",
            "Validation loss: 0.006044565\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2468938827514648\n",
            " \n",
            " \n",
            "Starting epoch: 72\n",
            "Validation loss: 0.0061336365\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.251823902130127\n",
            " \n",
            " \n",
            "Starting epoch: 73\n",
            "Validation loss: 0.0059709693\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.266098976135254\n",
            " \n",
            " \n",
            "Starting epoch: 74\n",
            "Validation loss: 0.0058197733\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.24436616897583\n",
            " \n",
            " \n",
            "Starting epoch: 75\n",
            "Validation loss: 0.0059819794\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2494089603424072\n",
            " \n",
            " \n",
            "Starting epoch: 76\n",
            "Validation loss: 0.006060911\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2666668891906738\n",
            " \n",
            " \n",
            "Starting epoch: 77\n",
            "Validation loss: 0.0060850834\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.3122169971466064\n",
            " \n",
            " \n",
            "Starting epoch: 78\n",
            "Validation loss: 0.0060653687\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2629542350769043\n",
            " \n",
            " \n",
            "Starting epoch: 79\n",
            "Validation loss: 0.006064016\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2360179424285889\n",
            " \n",
            " \n",
            "Starting epoch: 80\n",
            "Validation loss: 0.006050851\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2706470489501953\n",
            " \n",
            " \n",
            "Starting epoch: 81\n",
            "Validation loss: 0.006305018\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2713360786437988\n",
            " \n",
            " \n",
            "Starting epoch: 82\n",
            "Validation loss: 0.0060498305\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2419359683990479\n",
            " \n",
            " \n",
            "Starting epoch: 83\n",
            "Validation loss: 0.006037699\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2926900386810303\n",
            " \n",
            " \n",
            "Starting epoch: 84\n",
            "Validation loss: 0.0061081536\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2492880821228027\n",
            " \n",
            " \n",
            "Starting epoch: 85\n",
            "Validation loss: 0.006184658\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2910339832305908\n",
            " \n",
            " \n",
            "Starting epoch: 86\n",
            "Validation loss: 0.006195922\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2869749069213867\n",
            " \n",
            " \n",
            "Starting epoch: 87\n",
            "Validation loss: 0.006399699\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2996909618377686\n",
            " \n",
            " \n",
            "Starting epoch: 88\n",
            "Validation loss: 0.0064082257\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2797510623931885\n",
            " \n",
            " \n",
            "Starting epoch: 89\n",
            "Validation loss: 0.0063679437\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2891480922698975\n",
            " \n",
            " \n",
            "Starting epoch: 90\n",
            "Validation loss: 0.006397028\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2678160667419434\n",
            " \n",
            " \n",
            "Starting epoch: 91\n",
            "Validation loss: 0.0065682135\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.3077239990234375\n",
            " \n",
            " \n",
            "Starting epoch: 92\n",
            "Validation loss: 0.0067712143\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.285999059677124\n",
            " \n",
            " \n",
            "Starting epoch: 93\n",
            "Validation loss: 0.0065384465\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.3409628868103027\n",
            " \n",
            " \n",
            "Starting epoch: 94\n",
            "Validation loss: 0.0064780326\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.330070972442627\n",
            " \n",
            " \n",
            "Starting epoch: 95\n",
            "Validation loss: 0.006684057\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.309704065322876\n",
            " \n",
            " \n",
            "Starting epoch: 96\n",
            "Validation loss: 0.006283654\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2883191108703613\n",
            " \n",
            " \n",
            "Starting epoch: 97\n",
            "Validation loss: 0.0064197853\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2912230491638184\n",
            " \n",
            " \n",
            "Starting epoch: 98\n",
            "Validation loss: 0.0066160895\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.279756784439087\n",
            " \n",
            " \n",
            "Starting epoch: 99\n",
            "Validation loss: 0.006770741\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.289189100265503\n",
            " \n",
            " \n",
            "Starting epoch: 100\n",
            "Validation loss: 0.0069712545\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.324733018875122\n",
            " \n",
            " \n",
            "Starting epoch: 101\n",
            "Validation loss: 0.007095398\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2699620723724365\n",
            " \n",
            " \n",
            "Starting epoch: 102\n",
            "Validation loss: 0.007222301\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2766070365905762\n",
            " \n",
            " \n",
            "Starting epoch: 103\n",
            "Validation loss: 0.0072637354\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2605681419372559\n",
            " \n",
            " \n",
            "Starting epoch: 104\n",
            "Validation loss: 0.007029502\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.232234001159668\n",
            " \n",
            " \n",
            "Starting epoch: 105\n",
            "Validation loss: 0.0072734742\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2735819816589355\n",
            " \n",
            " \n",
            "Starting epoch: 106\n",
            "Validation loss: 0.007246235\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2456750869750977\n",
            " \n",
            " \n",
            "Starting epoch: 107\n",
            "Validation loss: 0.006855739\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.255810022354126\n",
            " \n",
            " \n",
            "Starting epoch: 108\n",
            "Validation loss: 0.0069388337\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.3179190158843994\n",
            " \n",
            " \n",
            "Starting epoch: 109\n",
            "Validation loss: 0.007300367\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.4753749370574951\n",
            " \n",
            " \n",
            "Starting epoch: 110\n",
            "Validation loss: 0.0076167975\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.3614509105682373\n",
            " \n",
            " \n",
            "Starting epoch: 111\n",
            "Validation loss: 0.0077320933\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2406837940216064\n",
            " \n",
            " \n",
            "Starting epoch: 112\n",
            "Validation loss: 0.0077657527\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2983918190002441\n",
            " \n",
            " \n",
            "Starting epoch: 113\n",
            "Validation loss: 0.00787142\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2617809772491455\n",
            " \n",
            " \n",
            "Starting epoch: 114\n",
            "Validation loss: 0.008200659\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.254727840423584\n",
            " \n",
            " \n",
            "Starting epoch: 115\n",
            "Validation loss: 0.008389383\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.283066987991333\n",
            " \n",
            " \n",
            "Starting epoch: 116\n",
            "Validation loss: 0.008498219\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.3141491413116455\n",
            " \n",
            " \n",
            "Starting epoch: 117\n",
            "Validation loss: 0.008964419\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2656631469726562\n",
            " \n",
            " \n",
            "Starting epoch: 118\n",
            "Validation loss: 0.009049681\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2758560180664062\n",
            " \n",
            " \n",
            "Starting epoch: 119\n",
            "Validation loss: 0.008733765\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2365450859069824\n",
            " \n",
            " \n",
            "Starting epoch: 120\n",
            "Validation loss: 0.008365851\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2505600452423096\n",
            " \n",
            " \n",
            "Starting epoch: 121\n",
            "Validation loss: 0.007870111\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2404351234436035\n",
            " \n",
            " \n",
            "Starting epoch: 122\n",
            "Validation loss: 0.007986405\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2569289207458496\n",
            " \n",
            " \n",
            "Starting epoch: 123\n",
            "Validation loss: 0.008365664\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2725670337677002\n",
            " \n",
            " \n",
            "Starting epoch: 124\n",
            "Validation loss: 0.008157971\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2995197772979736\n",
            " \n",
            " \n",
            "Starting epoch: 125\n",
            "Validation loss: 0.008440563\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.238887071609497\n",
            " \n",
            " \n",
            "Starting epoch: 126\n",
            "Validation loss: 0.008767699\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.240786075592041\n",
            " \n",
            " \n",
            "Starting epoch: 127\n",
            "Validation loss: 0.0089753885\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2661399841308594\n",
            " \n",
            " \n",
            "Starting epoch: 128\n",
            "Validation loss: 0.009279578\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2751898765563965\n",
            " \n",
            " \n",
            "Starting epoch: 129\n",
            "Validation loss: 0.009530768\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2476921081542969\n",
            " \n",
            " \n",
            "Starting epoch: 130\n",
            "Validation loss: 0.00953516\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.272392988204956\n",
            " \n",
            " \n",
            "Starting epoch: 131\n",
            "Validation loss: 0.009405783\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2514259815216064\n",
            " \n",
            " \n",
            "Starting epoch: 132\n",
            "Validation loss: 0.009897835\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2687578201293945\n",
            " \n",
            " \n",
            "Starting epoch: 133\n",
            "Validation loss: 0.009703874\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.268547773361206\n",
            " \n",
            " \n",
            "Starting epoch: 134\n",
            "Validation loss: 0.010121399\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2930591106414795\n",
            " \n",
            " \n",
            "Starting epoch: 135\n",
            "Validation loss: 0.009989688\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2858290672302246\n",
            " \n",
            " \n",
            "Starting epoch: 136\n",
            "Validation loss: 0.009871204\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.279047966003418\n",
            " \n",
            " \n",
            "Starting epoch: 137\n",
            "Validation loss: 0.010351237\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2506990432739258\n",
            " \n",
            " \n",
            "Starting epoch: 138\n",
            "Validation loss: 0.010763757\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2682690620422363\n",
            " \n",
            " \n",
            "Starting epoch: 139\n",
            "Validation loss: 0.011006087\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2786149978637695\n",
            " \n",
            " \n",
            "Starting epoch: 140\n",
            "Validation loss: 0.011187404\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.290673017501831\n",
            " \n",
            " \n",
            "Starting epoch: 141\n",
            "Validation loss: 0.011646434\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.276737928390503\n",
            " \n",
            " \n",
            "Starting epoch: 142\n",
            "Validation loss: 0.011857371\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2528069019317627\n",
            " \n",
            " \n",
            "Starting epoch: 143\n",
            "Validation loss: 0.012103611\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.292999029159546\n",
            " \n",
            " \n",
            "Starting epoch: 144\n",
            "Validation loss: 0.011357613\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.244706153869629\n",
            " \n",
            " \n",
            "Starting epoch: 145\n",
            "Validation loss: 0.011530693\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.241347074508667\n",
            " \n",
            " \n",
            "Starting epoch: 146\n",
            "Validation loss: 0.012809957\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2704119682312012\n",
            " \n",
            " \n",
            "Starting epoch: 147\n",
            "Validation loss: 0.012654513\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.3085579872131348\n",
            " \n",
            " \n",
            "Starting epoch: 148\n",
            "Validation loss: 0.010561501\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2980740070343018\n",
            " \n",
            " \n",
            "Starting epoch: 149\n",
            "Validation loss: 0.010645191\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2517750263214111\n",
            " \n",
            " \n",
            "Starting epoch: 150\n",
            "Validation loss: 0.010476496\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2763919830322266\n",
            " \n",
            " \n",
            "Starting epoch: 151\n",
            "Validation loss: 0.011847977\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2973840236663818\n",
            " \n",
            " \n",
            "Starting epoch: 152\n",
            "Validation loss: 0.011741492\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.5441999435424805\n",
            " \n",
            " \n",
            "Starting epoch: 153\n",
            "Validation loss: 0.012412105\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2820029258728027\n",
            " \n",
            " \n",
            "Starting epoch: 154\n",
            "Validation loss: 0.012211909\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.261976957321167\n",
            " \n",
            " \n",
            "Starting epoch: 155\n",
            "Validation loss: 0.011955757\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2756130695343018\n",
            " \n",
            " \n",
            "Starting epoch: 156\n",
            "Validation loss: 0.012207539\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2621800899505615\n",
            " \n",
            " \n",
            "Starting epoch: 157\n",
            "Validation loss: 0.012730201\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.3047900199890137\n",
            " \n",
            " \n",
            "Starting epoch: 158\n",
            "Validation loss: 0.013382077\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2557590007781982\n",
            " \n",
            " \n",
            "Starting epoch: 159\n",
            "Validation loss: 0.013728506\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.3051741123199463\n",
            " \n",
            " \n",
            "Starting epoch: 160\n",
            "Validation loss: 0.013967047\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2788281440734863\n",
            " \n",
            " \n",
            "Starting epoch: 161\n",
            "Validation loss: 0.014218775\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.314574956893921\n",
            " \n",
            " \n",
            "Starting epoch: 162\n",
            "Validation loss: 0.014449098\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.267514944076538\n",
            " \n",
            " \n",
            "Starting epoch: 163\n",
            "Validation loss: 0.014794779\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.3155131340026855\n",
            " \n",
            " \n",
            "Starting epoch: 164\n",
            "Validation loss: 0.015173397\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.265043020248413\n",
            " \n",
            " \n",
            "Starting epoch: 165\n",
            "Validation loss: 0.0155405225\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2502028942108154\n",
            " \n",
            " \n",
            "Starting epoch: 166\n",
            "Validation loss: 0.015865287\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2362539768218994\n",
            " \n",
            " \n",
            "Starting epoch: 167\n",
            "Validation loss: 0.016106917\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2723917961120605\n",
            " \n",
            " \n",
            "Starting epoch: 168\n",
            "Validation loss: 0.016346708\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.239260196685791\n",
            " \n",
            " \n",
            "Starting epoch: 169\n",
            "Validation loss: 0.016498439\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2381579875946045\n",
            " \n",
            " \n",
            "Starting epoch: 170\n",
            "Validation loss: 0.01682442\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2784650325775146\n",
            " \n",
            " \n",
            "Starting epoch: 171\n",
            "Validation loss: 0.017092926\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2847299575805664\n",
            " \n",
            " \n",
            "Starting epoch: 172\n",
            "Validation loss: 0.017340973\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2548511028289795\n",
            " \n",
            " \n",
            "Starting epoch: 173\n",
            "Validation loss: 0.017632566\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.216156005859375\n",
            " \n",
            " \n",
            "Starting epoch: 174\n",
            "Validation loss: 0.018094053\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2637951374053955\n",
            " \n",
            " \n",
            "Starting epoch: 175\n",
            "Validation loss: 0.01848796\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2691731452941895\n",
            " \n",
            " \n",
            "Starting epoch: 176\n",
            "Validation loss: 0.017879773\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.245162010192871\n",
            " \n",
            " \n",
            "Starting epoch: 177\n",
            "Validation loss: 0.0182532\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2302398681640625\n",
            " \n",
            " \n",
            "Starting epoch: 178\n",
            "Validation loss: 0.014442429\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2504041194915771\n",
            " \n",
            " \n",
            "Starting epoch: 179\n",
            "Validation loss: 0.015751902\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.25370192527771\n",
            " \n",
            " \n",
            "Starting epoch: 180\n",
            "Validation loss: 0.016277494\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2390060424804688\n",
            " \n",
            " \n",
            "Starting epoch: 181\n",
            "Validation loss: 0.01623913\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2798891067504883\n",
            " \n",
            " \n",
            "Starting epoch: 182\n",
            "Validation loss: 0.016075302\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2573931217193604\n",
            " \n",
            " \n",
            "Starting epoch: 183\n",
            "Validation loss: 0.01699446\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2683780193328857\n",
            " \n",
            " \n",
            "Starting epoch: 184\n",
            "Validation loss: 0.017925425\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2512969970703125\n",
            " \n",
            " \n",
            "Starting epoch: 185\n",
            "Validation loss: 0.018114164\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2542870044708252\n",
            " \n",
            " \n",
            "Starting epoch: 186\n",
            "Validation loss: 0.017667748\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.254060983657837\n",
            " \n",
            " \n",
            "Starting epoch: 187\n",
            "Validation loss: 0.017257245\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.3048820495605469\n",
            " \n",
            " \n",
            "Starting epoch: 188\n",
            "Validation loss: 0.017283546\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2827770709991455\n",
            " \n",
            " \n",
            "Starting epoch: 189\n",
            "Validation loss: 0.016267134\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2606980800628662\n",
            " \n",
            " \n",
            "Starting epoch: 190\n",
            "Validation loss: 0.015759163\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2440950870513916\n",
            " \n",
            " \n",
            "Starting epoch: 191\n",
            "Validation loss: 0.016413948\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2556500434875488\n",
            " \n",
            " \n",
            "Starting epoch: 192\n",
            "Validation loss: 0.01679161\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2701258659362793\n",
            " \n",
            " \n",
            "Starting epoch: 193\n",
            "Validation loss: 0.016859792\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2477638721466064\n",
            " \n",
            " \n",
            "Starting epoch: 194\n",
            "Validation loss: 0.017458227\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2716200351715088\n",
            " \n",
            " \n",
            "Starting epoch: 195\n",
            "Validation loss: 0.017142648\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.5234379768371582\n",
            " \n",
            " \n",
            "Starting epoch: 196\n",
            "Validation loss: 0.018000772\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2376530170440674\n",
            " \n",
            " \n",
            "Starting epoch: 197\n",
            "Validation loss: 0.01852163\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2315030097961426\n",
            " \n",
            " \n",
            "Starting epoch: 198\n",
            "Validation loss: 0.018939767\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2364120483398438\n",
            " \n",
            " \n",
            "Starting epoch: 199\n",
            "Validation loss: 0.019264987\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2502281665802002\n",
            " \n",
            " \n",
            "Starting epoch: 200\n",
            "Validation loss: 0.019636538\n",
            "Best validation loss: 0.005739489\n",
            "Time for epoch: 1.2547118663787842\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yhj9zQLO3fEf",
        "colab_type": "text"
      },
      "source": [
        "Cell to continue training if neessary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKjFgql32xHb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Continue training if necessary\n",
        "start_time = time()\n",
        "prev_time = time()\n",
        "for epoch in 501:600\n",
        "\n",
        "    # Print diagnostics\n",
        "    println(\"Time for epoch: \", time() - prev_time)\n",
        "    prev_time = time()\n",
        "    println(\" \")\n",
        "    println(\" \")\n",
        "    println(\"Starting epoch: \", epoch)\n",
        "\n",
        "    # Train model\n",
        "    Flux.train!(loss_reg, params, zip(train_xs, train_ys), opt, cb = Flux.throttle(evalcb, 5))\n",
        "\n",
        "    # Track losses\n",
        "    train_loss = (sum(loss_reg.(train_xs, train_ys))/length(train_xs)).data\n",
        "    val_loss   = (sum(loss_reg.(val_xs, val_ys))/length(val_xs)).data\n",
        "    push!(train_losses, train_loss)\n",
        "    push!(val_losses,   val_loss)\n",
        "    println(\"Validation loss: \", val_loss)\n",
        "    println(\"Best validation loss: \", best_val_loss)\n",
        "\n",
        "    # Save best model weights\n",
        "    if val_loss < best_val_loss\n",
        "      println(\"Found new best parameters!\")\n",
        "      best_params_a = deepcopy(Tracker.data.(Flux.params(accumulator)))\n",
        "      best_params_c = deepcopy(Tracker.data.(Flux.params(classifier)))\n",
        "      best_val_loss = val_loss\n",
        "    end\n",
        "\n",
        "end\n",
        "println(\"Time for epoch: \", time() - prev_time)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CP8pWPBCyQ9K",
        "colab_type": "code",
        "outputId": "c1d0471b-0c26-4b0e-c243-8c9a0f88598b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Load the weights which resulted in the best performance on the validation set\n",
        "Flux.loadparams!(accumulator, best_params_a)\n",
        "Flux.loadparams!(classifier, best_params_c)\n",
        "\n",
        "# Check the losses\n",
        "println(\"From the loaded model: \", (sum(loss.(val_xs, val_ys))/length(val_xs)).data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From the loaded model: 0.005739489\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwUunCTktoMO",
        "colab_type": "text"
      },
      "source": [
        "### 5.3.1 Convergence on the Training Set\n",
        "\n",
        "To test that the model capacity was sufficient to capture the complexity of the data, a simple version of the model was run for a large number of epochs. The structure of the MLP was two layers, with layer sizes `[60-60]` and `[60-5]`. \n",
        "\n",
        "The maximum accuracy on the training set reached after 600 epochs was **99.41%** which demonstrates that the model, even with a relatively simple MLP. The task from this point therefore was to avoid overfitting and achieve as high an accuracy on the validation set as possible. \n",
        "\n",
        "Interestingly, for this model, even after 600 epochs the accuracy on the validation set had not decreased too significantly, recording 37.24% on the validation set after 600 epochs.\n",
        "\n",
        "![Convergence Graph](https://drive.google.com/uc?export=view&id=1E9seRV5EdJbTz7KPVYSFcsTpYnaAhviD)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_i4ExhUbLZkw",
        "colab_type": "text"
      },
      "source": [
        "## 5.4 Plot the losses\n",
        "\n",
        "*Provide a plot of the loss on the training set and validation set for each epoch of training.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l37GGquKLkmb",
        "colab_type": "code",
        "outputId": "e00643fe-add2-4f3c-d385-8d13e7a8f635",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "source": [
        "# Plot training and validation losses\n",
        "plot([train_losses, val_losses], title=\"Loss\", xlabel=\"Epochs\", ylabel=\"Loss\", label = [\"Training\" \"Validation\" ],  size = (700, 300),legend=:bottomright)\n",
        "#plot(train_losses, title=\"Loss\", xlabel=\"Epochs\", ylabel=\"Loss\",  size = (700, 300),legend=:topright)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"700\" height=\"300\" viewBox=\"0 0 2800 1200\">\n<defs>\n  <clipPath id=\"clip6500\">\n    <rect x=\"0\" y=\"0\" width=\"2800\" height=\"1200\"/>\n  </clipPath>\n</defs>\n<path clip-path=\"url(#clip6500)\" d=\"\nM0 1200 L2800 1200 L2800 0 L0 0  Z\n  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<defs>\n  <clipPath id=\"clip6501\">\n    <rect x=\"560\" y=\"0\" width=\"1961\" height=\"1200\"/>\n  </clipPath>\n</defs>\n<path clip-path=\"url(#clip6500)\" d=\"\nM269.279 1025.62 L2752.76 1025.62 L2752.76 121.675 L269.279 121.675  Z\n  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<defs>\n  <clipPath id=\"clip6502\">\n    <rect x=\"269\" y=\"121\" width=\"2484\" height=\"905\"/>\n  </clipPath>\n</defs>\n<polyline clip-path=\"url(#clip6502)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  327.792,1025.62 327.792,121.675 \n  \"/>\n<polyline clip-path=\"url(#clip6502)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  916.462,1025.62 916.462,121.675 \n  \"/>\n<polyline clip-path=\"url(#clip6502)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  1505.13,1025.62 1505.13,121.675 \n  \"/>\n<polyline clip-path=\"url(#clip6502)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  2093.8,1025.62 2093.8,121.675 \n  \"/>\n<polyline clip-path=\"url(#clip6502)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  2682.47,1025.62 2682.47,121.675 \n  \"/>\n<polyline clip-path=\"url(#clip6502)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  269.279,1004.21 2752.76,1004.21 \n  \"/>\n<polyline clip-path=\"url(#clip6502)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  269.279,786.003 2752.76,786.003 \n  \"/>\n<polyline clip-path=\"url(#clip6502)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  269.279,567.801 2752.76,567.801 \n  \"/>\n<polyline clip-path=\"url(#clip6502)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  269.279,349.599 2752.76,349.599 \n  \"/>\n<polyline clip-path=\"url(#clip6502)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  269.279,131.397 2752.76,131.397 \n  \"/>\n<polyline clip-path=\"url(#clip6500)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  269.279,1025.62 2752.76,1025.62 \n  \"/>\n<polyline clip-path=\"url(#clip6500)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  269.279,1025.62 269.279,121.675 \n  \"/>\n<polyline clip-path=\"url(#clip6500)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  327.792,1025.62 327.792,1012.06 \n  \"/>\n<polyline clip-path=\"url(#clip6500)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  916.462,1025.62 916.462,1012.06 \n  \"/>\n<polyline clip-path=\"url(#clip6500)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  1505.13,1025.62 1505.13,1012.06 \n  \"/>\n<polyline clip-path=\"url(#clip6500)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  2093.8,1025.62 2093.8,1012.06 \n  \"/>\n<polyline clip-path=\"url(#clip6500)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  2682.47,1025.62 2682.47,1012.06 \n  \"/>\n<polyline clip-path=\"url(#clip6500)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  269.279,1004.21 306.531,1004.21 \n  \"/>\n<polyline clip-path=\"url(#clip6500)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  269.279,786.003 306.531,786.003 \n  \"/>\n<polyline clip-path=\"url(#clip6500)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  269.279,567.801 306.531,567.801 \n  \"/>\n<polyline clip-path=\"url(#clip6500)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  269.279,349.599 306.531,349.599 \n  \"/>\n<polyline clip-path=\"url(#clip6500)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  269.279,131.397 306.531,131.397 \n  \"/>\n<g clip-path=\"url(#clip6500)\">\n<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 327.792, 1081.62)\" x=\"327.792\" y=\"1081.62\">0</text>\n</g>\n<g clip-path=\"url(#clip6500)\">\n<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 916.462, 1081.62)\" x=\"916.462\" y=\"1081.62\">50</text>\n</g>\n<g clip-path=\"url(#clip6500)\">\n<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1505.13, 1081.62)\" x=\"1505.13\" y=\"1081.62\">100</text>\n</g>\n<g clip-path=\"url(#clip6500)\">\n<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 2093.8, 1081.62)\" x=\"2093.8\" y=\"1081.62\">150</text>\n</g>\n<g clip-path=\"url(#clip6500)\">\n<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 2682.47, 1081.62)\" x=\"2682.47\" y=\"1081.62\">200</text>\n</g>\n<g clip-path=\"url(#clip6500)\">\n<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 241.279, 1021.71)\" x=\"241.279\" y=\"1021.71\">0.000</text>\n</g>\n<g clip-path=\"url(#clip6500)\">\n<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 241.279, 803.503)\" x=\"241.279\" y=\"803.503\">0.005</text>\n</g>\n<g clip-path=\"url(#clip6500)\">\n<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 241.279, 585.301)\" x=\"241.279\" y=\"585.301\">0.010</text>\n</g>\n<g clip-path=\"url(#clip6500)\">\n<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 241.279, 367.099)\" x=\"241.279\" y=\"367.099\">0.015</text>\n</g>\n<g clip-path=\"url(#clip6500)\">\n<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 241.279, 148.897)\" x=\"241.279\" y=\"148.897\">0.020</text>\n</g>\n<g clip-path=\"url(#clip6500)\">\n<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:84px; text-anchor:middle;\" transform=\"rotate(0, 1511.02, 73.2)\" x=\"1511.02\" y=\"73.2\">Loss</text>\n</g>\n<g clip-path=\"url(#clip6500)\">\n<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(0, 1511.02, 1163.48)\" x=\"1511.02\" y=\"1163.48\">Epochs</text>\n</g>\n<g clip-path=\"url(#clip6500)\">\n<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(-90, 81.2861, 573.647)\" x=\"81.2861\" y=\"573.647\">Loss</text>\n</g>\n<polyline clip-path=\"url(#clip6502)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  339.566,782.858 351.339,782.911 363.113,782.936 374.886,784.271 386.659,787.267 398.433,790.284 410.206,792.332 421.98,795.931 433.753,800.139 445.526,805.546 \n  457.3,811.53 469.073,821.295 480.846,826.287 492.62,837.148 504.393,848.141 516.167,827.361 527.94,823.157 539.713,840.148 551.487,848.289 563.26,857.808 \n  575.033,863.802 586.807,868.257 598.58,871.389 610.354,873.999 622.127,875.425 633.9,877.559 645.674,879.152 657.447,881.01 669.221,882.474 680.994,883.858 \n  692.767,885.028 704.541,885.253 716.314,886.739 728.087,886.921 739.861,886.264 751.634,888.89 763.408,890.044 775.181,891.155 786.954,891.591 798.728,892.973 \n  810.501,893.268 822.275,894.517 834.048,894.349 845.821,895.637 857.595,896.132 869.368,896.378 881.141,896.396 892.915,897.259 904.688,898.091 916.462,898.838 \n  928.235,898.352 940.008,899.07 951.782,900.405 963.555,900.974 975.328,901.868 987.102,902.338 998.875,902.541 1010.65,902.914 1022.42,902.349 1034.2,899.349 \n  1045.97,901.628 1057.74,903.399 1069.52,904.726 1081.29,905.956 1093.06,906.148 1104.84,905.391 1116.61,906.198 1128.38,905.931 1140.16,907.169 1151.93,908.196 \n  1163.7,908.717 1175.48,906.431 1187.25,908.941 1199.02,910.922 1210.8,912.029 1222.57,911.317 1234.34,912.444 1246.12,914.355 1257.89,914.566 1269.66,911.936 \n  1281.44,913.044 1293.21,916.184 1304.98,916.165 1316.76,918.402 1328.53,919.682 1340.3,919.947 1352.08,920.297 1363.85,922.968 1375.62,922.972 1387.4,921.009 \n  1399.17,926.112 1410.94,926.704 1422.72,923.29 1434.49,919.771 1446.26,920.996 1458.04,927.786 1469.81,929.125 1481.58,931.025 1493.36,933.292 1505.13,935.138 \n  1516.9,936.4 1528.68,936.21 1540.45,929.9 1552.22,933.428 1564,931.699 1575.77,919.348 1587.54,925.096 1599.32,928.156 1611.09,932.395 1622.86,936.089 \n  1634.64,941.317 1646.41,943.842 1658.18,947.477 1669.96,948.967 1681.73,949.995 1693.5,950.337 1705.28,949.915 1717.05,953.594 1728.82,947.665 1740.6,943.842 \n  1752.37,954.214 1764.15,957.016 1775.92,956.559 1787.69,960.691 1799.47,961.908 1811.24,963.746 1823.01,960.977 1834.79,954.662 1846.56,953.856 1858.33,961.335 \n  1870.11,967.207 1881.88,957.649 1893.65,962.116 1905.43,963.193 1917.2,962.996 1928.97,967.603 1940.75,969.635 1952.52,967.524 1964.29,962.17 1976.07,960.129 \n  1987.84,965.787 1999.61,964.056 2011.39,958.639 2023.16,962.197 2034.93,957.245 2046.71,954.05 2058.48,933.207 2070.25,956.867 2082.03,961.098 2093.8,971.24 \n  2105.57,972.362 2117.35,971.972 2129.12,979.677 2140.89,984.988 2152.67,985.776 2164.44,986.879 2176.21,988.398 2187.99,989.399 2199.76,990.204 2211.53,990.941 \n  2223.31,991.545 2235.08,992.054 2246.85,992.593 2258.63,993.104 2270.4,993.605 2282.17,994.071 2293.95,994.54 2305.72,994.923 2317.49,995.245 2329.27,995.544 \n  2341.04,995.763 2352.81,995.995 2364.59,996.207 2376.36,996.584 2388.13,996.733 2399.91,996.403 2411.68,988.741 2423.45,989.488 2435.23,992.071 2447,994.123 \n  2458.77,995.987 2470.55,996.284 2482.32,996.598 2494.09,995.649 2505.87,993.922 2517.64,986.745 2529.41,980.272 2541.19,975.79 2552.96,971.841 2564.73,986.805 \n  2576.51,992.612 2588.28,993.204 2600.06,996.758 2611.83,997.735 2623.6,998.318 2635.38,999.044 2647.15,999.421 2658.92,999.682 2670.7,999.862 2682.47,1000.04 \n  \n  \"/>\n<polyline clip-path=\"url(#clip6502)\" style=\"stroke:#e26f46; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  339.566,678.819 351.339,678.985 363.113,679.012 374.886,720.196 386.659,696.099 398.433,705.387 410.206,701.71 421.98,705.978 433.753,705.351 445.526,706.573 \n  457.3,710.283 469.073,712.595 480.846,726.809 492.62,711.057 504.393,719.913 516.167,712.206 527.94,703.234 539.713,719.207 551.487,729.481 563.26,729.001 \n  575.033,734.69 586.807,732.726 598.58,732.934 610.354,734.191 622.127,731.658 633.9,735.759 645.674,733.049 657.447,737.465 669.221,736.904 680.994,738.634 \n  692.767,737.23 704.541,742.268 716.314,738.441 728.087,740.186 739.861,740.589 751.634,744.2 763.408,742.875 775.181,743.162 786.954,744.807 798.728,745.286 \n  810.501,745.198 822.275,745.985 834.048,747.517 845.821,745.998 857.595,745.861 869.368,747.796 881.141,748.514 892.915,747.089 904.688,745.6 916.462,748.105 \n  928.235,748.043 940.008,749.384 951.782,748.545 963.555,747.558 975.328,746.061 987.102,746.951 998.875,746.573 1010.65,746.357 1022.42,747.161 1034.2,748.284 \n  1045.97,753.732 1057.74,747.825 1069.52,752.039 1081.29,747.41 1093.06,746.15 1104.84,741.254 1116.61,742.581 1128.38,743.923 1140.16,742.436 1151.93,744.537 \n  1163.7,740.418 1175.48,736.531 1187.25,743.63 1199.02,750.228 1210.8,743.149 1222.57,739.705 1234.34,738.65 1246.12,739.51 1257.89,739.569 1269.66,740.144 \n  1281.44,729.052 1293.21,740.188 1304.98,740.718 1316.76,737.643 1328.53,734.304 1340.3,733.813 1352.08,724.92 1363.85,724.548 1375.62,726.306 1387.4,725.036 \n  1399.17,717.566 1410.94,708.707 1422.72,718.865 1434.49,721.501 1446.26,712.51 1458.04,729.984 1469.81,724.043 1481.58,715.476 1493.36,708.727 1505.13,699.977 \n  1516.9,694.559 1528.68,689.021 1540.45,687.213 1552.22,697.435 1564,686.788 1575.77,687.977 1587.54,705.018 1599.32,701.392 1611.09,685.614 1622.86,671.805 \n  1634.64,666.774 1646.41,665.305 1658.18,660.693 1669.96,646.325 1681.73,638.089 1693.5,633.339 1705.28,612.994 1717.05,609.273 1728.82,623.06 1740.6,639.116 \n  1752.37,660.75 1764.15,655.675 1775.92,639.124 1787.69,648.188 1799.47,635.856 1811.24,621.579 1823.01,612.516 1834.79,599.241 1846.56,588.279 1858.33,588.087 \n  1870.11,593.733 1881.88,572.26 1893.65,580.724 1905.43,562.503 1917.2,568.251 1928.97,573.422 1940.75,552.473 1952.52,534.47 1964.29,523.895 1976.07,515.982 \n  1987.84,495.95 1999.61,486.745 2011.39,475.999 2023.16,508.554 2034.93,501.001 2046.71,445.173 2058.48,451.957 2070.25,543.297 2082.03,539.645 2093.8,547.007 \n  2105.57,487.155 2117.35,491.802 2129.12,462.536 2140.89,471.272 2152.67,482.451 2164.44,471.463 2176.21,448.654 2187.99,420.206 2199.76,405.088 2211.53,394.677 \n  2223.31,383.692 2235.08,373.641 2246.85,358.555 2258.63,342.032 2270.4,326.01 2282.17,311.837 2293.95,301.293 2305.72,290.828 2317.49,284.206 2329.27,269.98 \n  2341.04,258.263 2352.81,247.438 2364.59,234.713 2376.36,214.573 2388.13,197.383 2399.91,223.924 2411.68,207.628 2423.45,373.932 2435.23,316.786 2447,293.849 \n  2458.77,295.523 2470.55,302.672 2482.32,262.56 2494.09,221.932 2505.87,213.695 2517.64,233.177 2529.41,251.092 2541.19,249.944 2552.96,294.301 2564.73,316.469 \n  2576.51,287.894 2588.28,271.412 2600.06,268.437 2611.83,242.321 2623.6,256.093 2635.38,218.644 2647.15,195.914 2658.92,177.666 2670.7,163.473 2682.47,147.258 \n  \n  \"/>\n<path clip-path=\"url(#clip6500)\" d=\"\nM2177.4 857.619 L2668.76 857.619 L2668.76 676.179 L2177.4 676.179  Z\n  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<polyline clip-path=\"url(#clip6500)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  2177.4,857.619 2668.76,857.619 2668.76,676.179 2177.4,676.179 2177.4,857.619 \n  \"/>\n<polyline clip-path=\"url(#clip6500)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  2205.4,736.659 2373.4,736.659 \n  \"/>\n<g clip-path=\"url(#clip6500)\">\n<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 2401.4, 754.159)\" x=\"2401.4\" y=\"754.159\">Training</text>\n</g>\n<polyline clip-path=\"url(#clip6500)\" style=\"stroke:#e26f46; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  2205.4,797.139 2373.4,797.139 \n  \"/>\n<g clip-path=\"url(#clip6500)\">\n<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 2401.4, 814.639)\" x=\"2401.4\" y=\"814.639\">Validation</text>\n</g>\n</svg>\n"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0WR_9_doJAb",
        "colab_type": "code",
        "outputId": "55e73610-dff1-4879-ac2b-04eb74bf439c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Find best validation\n",
        "minLoss = findmin(val_losses)[1]\n",
        "minEpoch = findmin(val_losses)[2]\n",
        "println(\"Best validation loss of $minLoss occurred at epoch $minEpoch\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best validation loss of 0.005739489 occurred at epoch 61\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVNtScVUM4PE",
        "colab_type": "text"
      },
      "source": [
        "## 5.5 Final Accuracy\n",
        "\n",
        "*Provide the final accuracy on the training, validation, and test set.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1R_7bGrAM-DV",
        "colab_type": "code",
        "outputId": "0e506b0c-b284-475b-b12d-9a761cec8977",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "function accuracyBatch(xdata, ydata)\n",
        "  Flux.testmode!(classifier)\n",
        "    c = 0\n",
        "    n = 0\n",
        "    for batch in 1:length(xdata)\n",
        "      xbatch = xdata[batch]\n",
        "      n_words = size(xbatch)[3]\n",
        "      Flux.reset!(accumulator)\n",
        "      for w in 1:n_words-1\n",
        "          accumulator(xbatch[:,:,w])\n",
        "      end\n",
        "      output = (classifier(accumulator(xbatch[:,:,n_words]))).data\n",
        "      predicted = []\n",
        "      for x in 1:size(output)[2]\n",
        "          predicted = findmax(output[:,x])[2]\n",
        "          actual = findmax(ydata[batch][:,x])[2]\n",
        "          if predicted == actual\n",
        "              c += 1\n",
        "          end\n",
        "          n += 1\n",
        "      end\n",
        "    end\n",
        "    return (c/n)*100\n",
        "end"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "accuracyBatch (generic function with 1 method)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7Z-qfywNoTL",
        "colab_type": "code",
        "outputId": "de3d5b3e-f561-480e-bf12-dc900482af7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "println(\"Validation accuracy: \", accuracyBatch(cpu.(val_xs), cpu.(val_ys)))\n",
        "println(\"Test accuracy: \", accuracyBatch(cpu.(test_xs), cpu.(test_ys)))\n",
        "println(\"Train accuracy: \", accuracyBatch(cpu.(train_xs), cpu.(train_ys)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation accuracy: 41.507720254314265\n",
            "Test accuracy: 41.40271493212669\n",
            "Train accuracy: 47.12078651685393\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LVIL-ylN7lf",
        "colab_type": "text"
      },
      "source": [
        "## 5.6 Sample classifications\n",
        "\n",
        "*Provide a selection of the classification decisions of the final model for 5 opinionated sentences from movies reviews that you find online – perhaps for a movie you liked, or did not like? Remember to provide a links to the reviews from which you selected the sentences.*\n",
        "\n",
        "Five sentences have been chosen from the internet - three from Rotten Tomatoes' list *Top 100 Movies of All Time*, and two bad from Wikipedia's list *List of films considered the worst*.\n",
        "\n",
        "**Dunkirk**\n",
        "\"One of the most captivating and compelling films of the year so far.\"<br/>https://www.rottentomatoes.com/m/dunkirk_2017\n",
        "\n",
        "**Mad Max: Fury Road**: \"Arty, gorgeous, exciting, compelling, and poignant all at once.\"<br/>https://www.rottentomatoes.com/m/mad_max_fury_road/\n",
        "\n",
        "**Thor: Ragnarok:**\n",
        "\"A great film that will definitely entertain you and keep a smile on your face.\"<br/>https://www.rottentomatoes.com/m/thor_ragnarok_2017\n",
        "\n",
        "**Birdemic: Shock and Terror:**\n",
        "\"It is, truly, one of the worst films ever made\"<br/>https://www.huffpost.com/entry/review-embirdemic-shock-a_b_491140\n",
        "\n",
        "**Meet the Spartans**\n",
        "\"This was the worst movie I've ever seen, so bad that I hesitate to label it a 'movie' and thus reflect shame upon the entire medium of film.\"<br/>https://www.rottentomatoes.com/m/meet_the_spartans"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgDjkFTyw_tZ",
        "colab_type": "code",
        "outputId": "01497611-6580-4a4d-8e85-31d8f01cafb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Predict a rating based on the embedded review\n",
        "function predictedRating(review)\n",
        "    n_words = size(review)[2]\n",
        "    Flux.reset!(accumulator)\n",
        "    for w in 1:n_words-1\n",
        "        accumulator(review[:,w])\n",
        "    end\n",
        "    return findmax(classifier(accumulator(review[:,n_words])))[2]\n",
        "end\n",
        "\n",
        "# Print out a review\n",
        "function printReview(review)\n",
        "    for word in review\n",
        "        print(word, \" \")\n",
        "    end\n",
        "    println(\" \" )\n",
        "end"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "printReview (generic function with 1 method)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5haOoTBOJBr",
        "colab_type": "code",
        "outputId": "4c25d57e-8c63-42a1-dfbe-4fc414cff5c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "dunkirk = [\"One\", \"of\", \"the\", \"most\", \"captivating\", \"and\", \"compelling\", \"films\", \"of\", \"the\", \"year\", \"so\", \"far\", \".\"]\n",
        "madmax = [\"Arty\", \",\", \"gorgeous\", \",\", \"exciting\", \",\", \"compelling\", \",\", \"and\", \"poignant\", \"all\", \"at\", \"once\", \".\"]\n",
        "thor = [\"A\", \"great\", \"film\", \"that\", \"will\", \"definitely\", \"entertain\", \"you\", \"and\", \"keep\", \"a\", \"smile\", \"on\", \"your\", \"face\", \".\"]\n",
        "spartans = [\"This\", \"was\", \"the\", \"worst\", \"movie\", \"I\", \"'ve\", \"ever\", \"seen\", \",\", \"so\", \"bad\", \"that\", \"I\", \"hesitate\", \"to\", \"label\", \"it\", \"a\", \"'\",\" movie\", \"'\", \"and\", \"thus\", \"reflect\", \"shame\", \"upon\", \"the\", \"entire\", \"medium\", \"of\", \"film\", \".\"]\n",
        "santa = [\"The\", \"plot\", \",\", \"such\", \"as\", \"it\", \"is\", \",\", \"proves\", \"it\", \"is\", \"possible\", \"to\", \"insult\", \"the\", \"intelligence\", \"of\", \"a\", \"three\", \"-\", \"year\", \"-\", \"old\"]\n",
        "names = [\"Dunkirk\", \"Max Max: Fury Road\", \"Thor: Ragnarok\", \"Meet the Spartans\", \"Santa Claus Conquers the Martians\"]\n",
        "reviews = [dunkirk, madmax, thor, spartans, santa]\n",
        "\n",
        "# Remove stopwords\n",
        "sampleReviews = removeStopwords(reviews)\n",
        "\n",
        "# Embed using word2vec\n",
        "reviewsData = gpu.(generateDataBatch(sampleReviews,500))\n",
        "\n",
        "# Predict rating: \n",
        "for i in 1:5\n",
        "    println(\" \")\n",
        "    println(names[i], \": \")\n",
        "    printReview(reviews[i])\n",
        "    println(\"Predicted rating: \", predictedRating(reviewsData[1][:,i,:]))\n",
        "end"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1..  \n",
            " \n",
            "Dunkirk: \n",
            "One of the most captivating and compelling films of the year so far .  \n",
            "Predicted rating: 5\n",
            " \n",
            "Max Max: Fury Road: \n",
            "Arty , gorgeous , exciting , compelling , and poignant all at once .  \n",
            "Predicted rating: 5\n",
            " \n",
            "Thor: Ragnarok: \n",
            "A great film that will definitely entertain you and keep a smile on your face .  \n",
            "Predicted rating: 4\n",
            " \n",
            "Meet the Spartans: \n",
            "This was the worst movie I 've ever seen , so bad that I hesitate to label it a '  movie ' and thus reflect shame upon the entire medium of film .  \n",
            "Predicted rating: 1\n",
            " \n",
            "Santa Claus Conquers the Martians: \n",
            "The plot , such as it is , proves it is possible to insult the intelligence of a three - year - old  \n",
            "Predicted rating: 2\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}